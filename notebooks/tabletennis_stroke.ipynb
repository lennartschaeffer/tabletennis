{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### imports\n"
      ],
      "metadata": {
        "id": "RXsmcQR0RI9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FOMTC2iBPjce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define transforms"
      ],
      "metadata": {
        "id": "JNTzMX8mRbhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "WZzd9zSrRUnp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load dataset, this uses ImageFolder so that forehand and backhand will automatically become the labels"
      ],
      "metadata": {
        "id": "mOJ-8XMKRkq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset', transform=transform)"
      ],
      "metadata": {
        "id": "GYzXTgO8Rdm5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create train, validation, test sets"
      ],
      "metadata": {
        "id": "KhXDeNS0Ru0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define lengths\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size  # remaining 15%\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d57dHQTNSOer",
        "outputId": "f465046e-cc3e-4692-8504-efad89a5ba81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['backhand', 'forehand']\n",
            "Train: 682, Val: 146, Test: 147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define the model"
      ],
      "metadata": {
        "id": "SW2zeFMrScX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pretrained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: forehand/backhand\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yDrLyYNSdoB",
        "outputId": "7cd07b3e-46ab-497f-8c6d-96e4496b92d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 192MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optimizer, loss"
      ],
      "metadata": {
        "id": "DRQGFTbvTI3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "kOz-b5R8TKY4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "SeSpVdMVTf9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3HGll4cTiYW",
        "outputId": "41a26f72-a1ad-4461-8bd3-d4123df4e707"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.0457, Accuracy: 0.9824\n",
            "Validation Accuracy: 1.0000\n",
            "Epoch 2/5 - Loss: 0.0007, Accuracy: 1.0000\n",
            "Validation Accuracy: 1.0000\n",
            "Epoch 3/5 - Loss: 0.0002, Accuracy: 1.0000\n",
            "Validation Accuracy: 1.0000\n",
            "Epoch 4/5 - Loss: 0.0002, Accuracy: 1.0000\n",
            "Validation Accuracy: 1.0000\n",
            "Epoch 5/5 - Loss: 0.0003, Accuracy: 1.0000\n",
            "Validation Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### test set"
      ],
      "metadata": {
        "id": "uTNDO7dCT-Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v2Ve_BwT_k2",
        "outputId": "d14b5d85-56dc-43e2-dbcf-d25337fa5dd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"forehand_backhand_model.pth\")"
      ],
      "metadata": {
        "id": "_j1q1H6c2VEm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"forehand_backhand_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9KTjwlLi2kqx",
        "outputId": "c4357370-7cfb-4651-f30f-b463a921559e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a695d829-8294-4bd2-a619-986ff12daac8\", \"forehand_backhand_model.pth\", 44791070)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test with a new image"
      ],
      "metadata": {
        "id": "PRFf4RRv7s1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Same transform as training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your new image\n",
        "img_path = \"/content/forehand_blackshirt.MOV_11.jpg\"\n",
        "# img_path = \"/content/fh_test.jpeg\"\n",
        "\n",
        "image = Image.open(img_path).convert('RGB')  # ensure 3 channels\n",
        "\n",
        "# Apply transform and add batch dimension\n",
        "image = transform(image).unsqueeze(0).to(device)  # [1, C, H, W]\n",
        "\n",
        "# Make prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    predicted_class = dataset.classes[predicted.item()]  # dataset from ImageFolder\n",
        "\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Defw-1nK3dtm",
        "outputId": "d967cfa4-ab3a-458a-b7c3-5fe27fbb8912"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-7.2700,  5.1628]], device='cuda:0')\n",
            "Predicted class: forehand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### do a test with a video, frame by frame"
      ],
      "metadata": {
        "id": "5JYZYASE7nT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "videoPath = \"/content/fh_bh_testvid.MOV\"\n",
        "\n",
        "vidcap = cv2.VideoCapture(videoPath)\n",
        "frame_count = 0\n",
        "\n",
        "while True:\n",
        "    success, frame = vidcap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    if frame_count % 10 == 0:  # every 10th frame\n",
        "        # Convert OpenCV BGR -> RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        # Convert NumPy array to PIL Image\n",
        "        image = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Apply transform and predict\n",
        "        image = transform(image).unsqueeze(0).to(device)  # [1, C, H, W]\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted_class = dataset.classes[predicted.item()]\n",
        "\n",
        "        print(f\"Frame {frame_count}: {predicted_class}\")\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "vidcap.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXqcN32V7nFw",
        "outputId": "7314dc12-f5f6-4f87-b494-1359afac7d2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0: forehand\n",
            "Frame 10: forehand\n",
            "Frame 20: forehand\n",
            "Frame 30: forehand\n",
            "Frame 40: forehand\n",
            "Frame 50: forehand\n",
            "Frame 60: forehand\n",
            "Frame 70: backhand\n",
            "Frame 80: backhand\n",
            "Frame 90: backhand\n",
            "Frame 100: backhand\n",
            "Frame 110: backhand\n",
            "Frame 120: backhand\n",
            "Frame 130: forehand\n",
            "Frame 140: forehand\n",
            "Frame 150: forehand\n",
            "Frame 160: forehand\n",
            "Frame 170: forehand\n",
            "Frame 180: forehand\n",
            "Frame 190: forehand\n",
            "Frame 200: forehand\n",
            "Frame 210: backhand\n",
            "Frame 220: backhand\n",
            "Frame 230: backhand\n",
            "Frame 240: backhand\n",
            "Frame 250: backhand\n",
            "Frame 260: forehand\n",
            "Frame 270: forehand\n",
            "Frame 280: forehand\n",
            "Frame 290: forehand\n",
            "Frame 300: forehand\n",
            "Frame 310: forehand\n",
            "Frame 320: backhand\n",
            "Frame 330: backhand\n",
            "Frame 340: backhand\n",
            "Frame 350: backhand\n",
            "Frame 360: backhand\n",
            "Frame 370: forehand\n",
            "Frame 380: forehand\n",
            "Frame 390: forehand\n",
            "Frame 400: forehand\n",
            "Frame 410: forehand\n",
            "Frame 420: forehand\n"
          ]
        }
      ]
    }
  ]
}